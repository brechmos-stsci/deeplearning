{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements.txt file for virtualenv\n",
    "\n",
    "Keras\n",
    "numpy\n",
    "astropy\n",
    "jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# convlulation layres to help train on image data\n",
    "from keras.layers import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "\n",
    "# optimizers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# core layers\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "def rebin(a, shape):\n",
    "    sh = shape[0],a.shape[0]//shape[0],shape[1],a.shape[1]//shape[1]\n",
    "    return a.reshape(sh).mean(-1).mean(1)\n",
    "\n",
    "def blockshaped(arr, nrows, ncols):\n",
    "    \"\"\"\n",
    "    Return an array of shape (n, nrows, ncols) where\n",
    "    n * nrows * ncols = arr.size\n",
    "\n",
    "    If arr is a 2D array, the returned array should look like n subblocks with\n",
    "    each subblock preserving the \"physical\" layout of arr.\n",
    "    \"\"\"\n",
    "    h, w = arr.shape\n",
    "    return (arr.reshape(h//nrows, nrows, -1, ncols)\n",
    "               .swapaxes(1,2)\n",
    "               .reshape(-1, nrows, ncols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display = True\n",
    "labels = ['750', '850']\n",
    "path='/Users/crjones/Documents/Science/HargisDDRF/astroNN/data/wl_maps'\n",
    "degrade=8\n",
    "nct = 9\n",
    "imsize=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 0  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.750_4096xy_0009r_0029p_0100z_og.gre.fit\n",
      "i: 0  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0001r_0029p_0100z_og.gre.fit\n",
      "i: 1  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0002r_0029p_0100z_og.gre.fit\n",
      "i: 2  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0003r_0029p_0100z_og.gre.fit\n",
      "i: 3  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0004r_0029p_0100z_og.gre.fit\n",
      "i: 4  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0005r_0029p_0100z_og.gre.fit\n",
      "i: 5  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0006r_0029p_0100z_og.gre.fit\n",
      "i: 6  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0007r_0029p_0100z_og.gre.fit\n",
      "i: 7  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0008r_0029p_0100z_og.gre.fit\n",
      "i: 8  j: 1  name: smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.850_4096xy_0009r_0029p_0100z_og.gre.fit\n"
     ]
    }
   ],
   "source": [
    "imgs = np.zeros([2048//degrade, 2048//degrade, nct, len(labels)])\n",
    "data = []\n",
    "for j, label in enumerate(labels):\n",
    "    for i in range(nct):\n",
    "        filename = os.path.join(path, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit')\n",
    "        if display:\n",
    "           print(\"i: %d  j: %d  name: %s\" % (i, j, 'smoothWL-conv_m-512b240_Om0.260_Ol0.740_w-1.000_ns0.960_si0.'+label+'_4096xy_000'+ np.str(i+1) +'r_0029p_0100z_og.gre.fit'))\n",
    "\n",
    "        # Read in the data and put into the imgs.\n",
    "        f = fits.open(filename)\n",
    "        \n",
    "        tt = blockshaped(rebin(f[0].data/np.percentile(f[0].data,99.9)*100, [2048//degrade, 2048//degrade]), imsize, imsize)\n",
    "        \n",
    "        for smimg in tt:\n",
    "            data.append( (int(label), i, smimg) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices_training = np.random.choice(range(len(data)), 1000)\n",
    "#inidices_validation = []\n",
    "#indicies_testing = list(set(range(len(data))) - set(indices_training))\n",
    "\n",
    "# \n",
    "indices_training = [ii for ii,x in enumerate(data) if x[1] < 7]\n",
    "indices_validation = [ii for ii,x in enumerate(data) if x[1] == 7]\n",
    "indicies_testing = [ii for ii,x in enumerate(data) if x[1] == 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotate, transpose the data to create new sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the 8x8 sub images from the image just read in\n",
    "training_imgs = []\n",
    "training_labels = []\n",
    "\n",
    "# Setup the shift variables (taken from 4_conv_WL notebook)\n",
    "exp_cut, exp_nshift = 3, 3\n",
    "npanelx = 2**exp_cut\n",
    "panelw = 2048//(degrade*npanelx)\n",
    "nshift = 2**exp_nshift - 1\n",
    "shiftw = panelw // npanelx\n",
    "\n",
    "# Grab the list of 8x8 sub arrays\n",
    "#temp_training_imgs = [im for ii in indices_training for im in blockshaped(data[ii][2],imsize,imsize) ]\n",
    "#temp_labels = [im for ii in indices_training for im in data[ii][0]]\n",
    "\n",
    "temp_training_imgs = []\n",
    "temp_labels = []\n",
    "for ii in indices_training:\n",
    "    temp_training_imgs.append(data[ii][2])\n",
    "    temp_labels.append(data[ii][0])\n",
    "\n",
    "# Add in all the shifts\n",
    "temp_training_imgs.extend([np.roll(x, (r,c), axis=(0,1)) \n",
    "     for x in temp_training_imgs \n",
    "     for r in [1,  5,  9, 13, 17, 21, 25] \n",
    "     for c in [1,  5,  9, 13, 17, 21, 25]\n",
    "])\n",
    "temp_labels.extend([tl \n",
    "                    for tl in temp_labels \n",
    "                    for r in [1,  5,  9, 13, 17, 21, 25] \n",
    "                    for c in [1,  5,  9, 13, 17, 21, 25]\n",
    "                   ])\n",
    "\n",
    "# Now create all the flips, rotations, transpositions etc\n",
    "training_imgs.extend(temp_training_imgs)  # sub images\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([x.T for x in temp_training_imgs]) # transposed sub images\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([np.rot90(x) for x in temp_training_imgs]) # rotated sub images\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([np.rot90(x, k=2) for x in temp_training_imgs]) # rotated twice sub images\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([np.rot90(x, k=3) for x in temp_training_imgs]) # rotated three x sub images\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([x[::-1] for x in temp_training_imgs]) # flip ud\n",
    "training_labels.extend(temp_labels)\n",
    "\n",
    "training_imgs.extend([x[:,::-1] for x in temp_training_imgs]) # flip lr\n",
    "training_labels.extend(temp_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make them into a 3D array and 1D array\n",
    "X_train = np.stack(training_imgs)\n",
    "y_train = training_labels\n",
    "\n",
    "# Add dimension to X_train for keras\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, imsize, imsize)\n",
    "\n",
    "# change y_train to labels of 0 and 1 (conversion of boolean to int)\n",
    "y_train = np.array(y_train) == 850\n",
    "y_train = np_utils.to_categorical(y_train, 2)\n",
    "\n",
    "# YIKES https://github.com/ml4a/ml4a-guides/issues/10  !!!!!!!!!!!\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the X_test and y_test with the '850' data\n",
    "X_validation = [data[ii][2] for ii in indices_validation]\n",
    "y_validation = [data[ii][0] for ii in indices_validation]\n",
    "\n",
    "# Now make them into a 3D array and 1D array\n",
    "X_validation = np.stack(X_validation)\n",
    "y_validation = np.array(y_validation)\n",
    "\n",
    "# Add dimension to X_train for keras\n",
    "X_validation = X_validation.reshape(X_validation.shape[0], 1, imsize, imsize)\n",
    "\n",
    "# change y_test to labels of 0 and 1 (conversion of boolean to int)\n",
    "y_validation = np.array(y_validation == 850)*1\n",
    "y_validation = np_utils.to_categorical(y_validation, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The Keras Model\n",
    "One note on this part. There was an issue, originally of setting the filter size in this step to anything larger than (1,1).  After Googling around it appears that it had to do with an issue in Keras (described here https://github.com/ml4a/ml4a-guides/issues/10). It seems the order of the image dimensions had a conflict.  With the 2 line fix immediately above this all runs fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# input is 4D tensor with shape: (samples, channels, rows, cols)\n",
    "# output is 4D tensor with shape: (samples, filters, new_rows, new_cols)\n",
    "model.add(Conv2D(32, (3, 3), strides=(3,3), activation='relu', input_shape=(1, imsize, imsize)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), strides=(3,3), activation='relu'))\n",
    "\n",
    "# MaxPooling2D is a way to reduce the number of parameters in our model by\n",
    "# sliding a 2x2 pooling filter across the previous layer and taking the max of\n",
    "# the 4 values in the 2x2 filter.\n",
    "model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "\n",
    "# https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer='rmsprop', metrics=['accuracy', 'mae', 'mape'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 313600 samples, validate on 128 samples\n",
      "Epoch 1/10\n",
      "313600/313600 [==============================] - 100s - loss: 0.6950 - acc: 0.5225 - mean_absolute_error: 0.4975 - mean_absolute_percentage_error: 248762814.1796 - val_loss: 0.6636 - val_acc: 0.7422 - val_mean_absolute_error: 0.4836 - val_mean_absolute_percentage_error: 241775076.0000\n",
      "Epoch 2/10\n",
      "313600/313600 [==============================] - 104s - loss: 0.6872 - acc: 0.5500 - mean_absolute_error: 0.4913 - mean_absolute_percentage_error: 245638803.2702 - val_loss: 0.6453 - val_acc: 0.7891 - val_mean_absolute_error: 0.4737 - val_mean_absolute_percentage_error: 236834392.0000\n",
      "Epoch 3/10\n",
      " 81440/313600 [======>.......................] - ETA: 78s - loss: 0.6818 - acc: 0.5664 - mean_absolute_error: 0.4850 - mean_absolute_percentage_error: 242517110.0794"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_validation, y_validation), epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the testing data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the X_test and y_test with the '850' data\n",
    "X_test = [data[ii][2] for ii in indicies_testing]\n",
    "y_test = [data[ii][0] for ii in indicies_testing]\n",
    "\n",
    "# Now make them into a 3D array and 1D array\n",
    "X_test = np.stack(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Add dimension to X_train for keras\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, imsize, imsize)\n",
    "\n",
    "# change y_test to labels of 0 and 1 (conversion of boolean to int)\n",
    "y_test = np.array(y_test == 850)*1\n",
    "y_test = np_utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/128 [======>.......................] - ETA: 0sloss=0.5580697655677795, acc=0.71875, mean_absolute_error=0.34731612354516983, mean_absolute_percentage_error=173658080.0\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "names = model.metrics_names\n",
    "print('{}={}, {}={}, {}={}, {}={}'.format(names[0], scores[0], names[1], scores[1], names[2], scores[2], names[3], scores[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = l1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 31.5, -0.5, 2.5)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD8CAYAAABekO4JAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOiKicAnUERGFS6COiCjc/we2720PRWTDwwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126ff42e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.vstack(ws[0][0]))\n",
    "plt.colorbar()\n",
    "plt.axis('auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
